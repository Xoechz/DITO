\chapter{Introduction}
\label{chap:introduction}

\section{Motivation}
\label{sec:i_motivation}

OpenTelemetry has established itself as a groundbreaking new standard in the field of observability~\citebooks{Young2024/22; , Young2021/8}.
It is replacing many vendor-specific solutions with a unified, standardized, extensible, open-source, 
and language-integrated framework.

While OpenTelemetry was initially designed with microservices in mind, its applicability extends far beyond that~\citebook{Boten2022}{4-15}.
Distributed systems of all kinds, including legacy systems, can benefit significantly from its capabilities~\citebook{Gomez_Blanco2023}{179}.

\section{Problem}
\label{sec:i_problem}

A major challenge in legacy systems, especially those that have evolved organically over time,
is the absence of a single source of truth~\citebook{Flanders2024}{427}.
Data entries, such as employee information, are often stored across multiple, loosely connected subsystems.
For instance, the data indicating which manager an employee reports to might reside in a
different subsystem than the one storing team assignments.
These structural issues are typically beyond the immediate control of developers.
Centralizing and unifying these data sources is a complex and time-consuming process
that requires a company-wide effort to address technical debt and correct historical inconsistencies.

To maintain operational continuity during this transition, data from legacy systems must be synchronized with the new system.
However, this migration is prone to mismatches, invalid states, and other errors due to the fragmented nature of the data.
While these issues may take time to resolve, they should not halt the entire migration process.

Developers and decision-makers need to stay informed about when, why, and for how long errors
occur, without having to manually sift through logs from multiple applications.
A centralized observability dashboard that aggregates logs, traces, and metrics can provide this visibility.

While basic telemetry collection is achievable using zero-code instrumentation and backends like Jaeger, Zipkin, Prometheus,
or Microsoft Application Insights, the challenge lies in the volume and complexity of the logs.
Traces, which are mostly underutilized, offer a more structured and insightful alternative~\citebook{Young2024}{28}.

\section{Goals}
\label{sec:i_goals}

The primary goal of this thesis can be split into two distinct parts:
(1) to generate detailed traces for individual data points during data transfer jobs, and 
(2) to collect and reorganize these traces to visualize the flow of data through the system.

\subsection{Trace Creation}

Each data transfer job execution should generate a trace that includes child spans for every data point processed.
These spans will contain metadata such as transfer status, timestamp, data type, and a unique identifier.
The parent span will include contextual information like source, destination, and a flag indicating that the 
trace should be processed by a custom collector.

\subsection{Trace Collection and Regrouping}

OpenTelemetry allows telemetry data to be exported either directly to a backend or via a collector.
The extended collector developed in this thesis will intercept traces, extract spans based on data point identifiers,
and regroup them into new traces that represent the journey of individual data points.
These regrouped traces will then be sampled and exported.
Since jobs may run independently and at different times,
the collector must persist telemetry data to enable correlation across time.

\section{Methodology}
\label{sec:i_methodology}

This thesis will involve the development of a prototype system using two main technologies:

\begin{itemize}
    \item \textbf{.NET (C\#):} Used to implement the base system responsible for executing data transfer jobs and generating OpenTelemetry traces.
    \item \textbf{GO (Golang):} Used to extend an OpenTelemetry collector with a custom processor that processes, filters, and reorganizes traces.
\end{itemize}

\subsection{Base System Development}
A .NET-based application will be implemented to simulate data transfer jobs.
The application will be instrumented using the OpenTelemetry SDK to generate traces 
enriched with semantic events and metadata.
Each job execution will produce a trace with child spans for each data point,
including metadata such as transfer status, timestamp, data type, and a unique identifier.

\subsection{Custom Collector Implementation}
The OpenTelemetry collector will be expanded with a custom processor to reorganize traces.
This collector will:

\begin{itemize}
    \item Intercept and persist telemetry data.
    \item Extract spans based on data point identifiers.
    \item Regroup spans into new traces representing the journey of individual data points.
    \item Sample and export the reorganized traces to a telemetry backend.
\end{itemize}
The collector must support delayed correlation of spans, as jobs may run independently and at different times.

\subsection{Evaluation and Metrics}
Metrics will be generated from the reorganized traces to provide insights into:
\begin{itemize}
    \item Data quality and consistency.
    \item Frequency and duration of errors.
    \item System performance across different jobs and machines.
\end{itemize}
These metrics will be visualized using a telemetry backend such as Microsoft Application Insights or Prometheus.
