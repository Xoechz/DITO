\chapter{Expos√©}

\section{Motivation}

% \subsection{TO BE DELETED: NOTES}

% \begin{itemize}
% 	\item Current State of OTEL and a future of very probable mass adoption(with citations)
% 	\item OTEL not only in Microservices, but all distributed systems. Company wide observability
% 	\item Transition to the problem, mentioning services which transfer a lot of data
% \end{itemize}

% \subsection{ACTUAL TEXT}

OpenTelemetry has established itself as a groundbreaking new standard in the world of observability.
It is replacing many vendor specific solutions with a single, standardized, extensible, open source
and language integrated solution.

OpenTelemetry has been designed with microservices in mind, but it is not exclusive to microservices.
Distributed systems of all kinds, even legacy systems, can profit of OpenTelemetry. 

\section{Problem}

% \subsection{TO BE DELETED: NOTES}

% \begin{itemize}
% 	\item Fractured company software systems with multiple sources of truth.
% 	\item Merging and transferring data across multiple systems.
% 	\item Inconsistent data
% 	\item Notifying admins of the different single systems about data quality.
% 	\item Concrete Problem - try to explain our hell
% \end{itemize}

% \subsection{ACTUAL TEXT}

A big problem in legacy systems, especially naturally grown ones, is that often no single source of truth exists.
So data entries, like employee data, are often stored in multiple loosely connected sub systems.
For example the information which employee has which manager could be stored in another subsystem
as the information in which team an employee is working. Issues like these are often structural
and can not directly and immediately be influenced by the developers tasked with dealing with the system.
Unifying the data sources and saving them in a centralized system is a tedious and long undertaking,
requiring the whole companies effort to clean up past technical dept and incorrect data.

To keep the company operational while this is and to migrate the system step by step, 
the data from the legacy sources needs to be synced with the new system, but there are bound to be mismatches, 
invalid states and other errors, due to no single source of truth existing.
These issues can often take a long time to fix, but they should not stop the entire syncing process.
The developers and decision makers do however need to be informed when, for how long and why errors are happening,
without having to click through every applications logs.

To help getting through this chaos, all the observability data, meaning logs, traces and metrics, should be
collected to a single dashboard showing all the issues and the performance of the whole system.

This alone is easily achieved using simple zero-code instrumentation and a telemetry backend of your choice
like Jaeger, Zipkin, Prometheus or in this case Microsoft Application Insights. The problem with this solution
is that the logs that get collected are very difficult to search through, especially if the amount of data and
potential logs exceed a thousand entries per execution, while traces are almost unused.

\section{Goals}

% \subsection{TO BE DELETED: NOTES}

% \begin{itemize}
% 	\item Explain Traces and Metrics in the context of OpenTelemetry.
% 	\item Explain Collectors, their use, components and different types of Collectors.
% 	\item Write a Collector in GO and generate traces in a .NET Application to
% 	      \begin{itemize}
% 		      \item Provide an overview over the data quality in the system.
% 		      \item See where errors are happening and what their source could be.
% 		      \item Have traces for every data point travelling through a system, next to the traces per job execution.
% 		      \item Metrics which are able to be grouped by source, destination, entity being synced, job or machine executing the job.
% 	      \end{itemize}
% 	\item do that a bit more and a bit better lol
% \end{itemize}

% \subsection{ACTUAL TEXT}

The goal of this thesis can be split into two parts, the creation of traces for transferred data points
and the collection and regrouping of these traces to represent the flow of data through the system.
Creation is handled in the application code, while collection and regrouping is handled by a
custom OpenTelemetry collector.

\subsubsection{Creation}

A trace should be created for each execution of a data transfer job,
which contains child spans for every data point in this job.
Those child spans contain the status of the transfer, the time,
a type and an identification for the data point.
The parent trace contains information about source, destination and a flag, indicating that the custom collector should
work on this trace.

\subsubsection{Collection}

OpenTelemetry Traces, Metrics and Logs can be exported either directly to a telemetry backend, 
or to a proxy, called a collector. This collector gathers telemetry data from different sources,
performs operations like batching or filtering and then exports the telemetry data to different
backends.

The created traces are taken out of the normally exported data and are then regrouped based
on the data point identification. They are first split into the spans for each data point,
the spans are then grouped by their identification and then they get sampled and exported.

Due to the jobs being independent of each other, the telemetry data has to be saved
to be able to group jobs happening hours from each other.

\section{Methodology}

% \subsection{TO BE DELETED: NOTES}

% \begin{itemize}
% 	\item Explain Tech stack(.net, golang)
% 	\item Talk about Prototype and results
% \end{itemize}

% \subsection{ACTUAL TEXT} 
% Do gheat nu mea hea, generell gheat ois nu a weng umgschribm und mitn abstract obgstimmt.
% KI kau do ah nu wos doa und de biblipgraphie gheat ah nu eina

In the scope of this thesis a prototype will be created using C\verb|#| with .NET,
for the base system, and GO, for the collector.